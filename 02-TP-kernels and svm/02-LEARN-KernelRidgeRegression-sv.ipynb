{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Kernel methods and SVMs\n",
    "** APSTA - Ecole Centrale Nantes **\n",
    "\n",
    "** Diana Mateus **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTICIPANTS: **(Fill in your names)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kernel Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is an extension to ordinary least squares by adding a regularization\n",
    "term to the loss function. It is defined as\n",
    "\\begin{equation}\n",
    "\\min_{\\mathbf{w}} \\sum_{i=1}^n (y_i -  \\mathbf{x}_i^T \\mathbf{w} )^2\n",
    "+ \\lambda \\lVert \\mathbf{w} \\rVert_2^2 ,\n",
    "\\end{equation}\n",
    "where the value of $\\lambda > 0$ determines the amount of regularization. In this exercise we will rely on the derivation of Support Vector Machines to extend Ridge regression with the ```Kernel Trick```\n",
    "\n",
    "**a.** Replace $\\mathbf{w}$ with $\\sum_{i=1}^n \\alpha_i \\mathbf{x}_i$\n",
    "\n",
    "\n",
    "\n",
    "**b.** As in support vector machines, we can use the Kernel trick to make ridge regression\n",
    "non-linear and at the same time avoid explicitly transforming features. Specify $k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\phi(\\mathbf{x})^T\\phi(\\mathbf{x}^\\prime)$, to derive the objective function of Kernel Ridge Regression.\n",
    "\n",
    "\n",
    "\n",
    "**c.** Derive the the solution for the $\\alpha$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**d.** How would you use the result to make a new prediction?\n",
    "\n",
    "\n",
    "\n",
    "**e.** What are the main similarities and differences of KRR with the classification SVM derived in class?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wine quality prediction \n",
    "\n",
    "The wine quality dataset comes from the UCI Machine Learning Repository http://archive.ics.uci.edu/ml/index.php, and contains measurements and opinions for different variants of red and white wine. The goal of this part of the exercise  is to build a model capable to _predict the quality of a wine from the measurements_.\n",
    "\n",
    "To this end, implement your own version of Kernel Ridge Regresssion and compare it with the in-built SVR function from sklearn\n",
    "\n",
    "**a.** Run the ```Load and process``` block bellow to load the dataset into the ``wines_backup.csv`` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Load and Process \n",
    "# Saves the result in a single file in order to avoid downloading many times.\n",
    "# Shows the first 5 lines of the table\n",
    "\n",
    "if not os.path.exists(\"wines_backup.csv\"):\n",
    "    # if not exist, we create wines.csv which combines red and white wines into a single file\n",
    "    columns = [\"facidity\", \"vacidity\", \"citric\", \"sugar\", \"chlorides\", \"fsulfur\", \n",
    "               \"tsulfur\", \"density\", \"pH\", \"sulphates\", \"alcohol\", \"quality\"]\n",
    "    red = pandas.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "                         names=columns, sep=\";\", skiprows=1)\n",
    "    white = pandas.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\",\n",
    "                         names=columns, sep=\";\", skiprows=1)\n",
    "    red[\"color\"] = \"red\"\n",
    "    white[\"color\"] = \"white\"\n",
    "    wines = pandas.concat([white, red])\n",
    "    wines.to_csv(\"wines_backup.csv\", sep=\"\\t\", index=False)\n",
    "else:\n",
    "    wines = pandas.read_csv(\"wines_backup.csv\", sep=\"\\t\")\n",
    "    \n",
    "wines.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Split the dataset into train (80% of samples) and test (20% samples)**. Use the in-built sklearn function\n",
    "```train_test_split```\n",
    "\n",
    "``\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2,random_state=3)\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Data split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#build the data matrix from a subset of the available variables in the cvs file\n",
    "X = wines.as_matrix([\"facidity\", \"vacidity\", \"citric\", \"sugar\", \"chlorides\", \"fsulfur\", \n",
    "               \"tsulfur\", \"density\", \"pH\", \"sulphates\", \"alcohol\"])\n",
    "\n",
    "#make y the target value we want to predict\n",
    "y = wines.as_matrix(['quality']).ravel()\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "#Split the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2,random_state=3)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.  Implement your own version of Kernel Ridge Regressor**. To this end \n",
    "- fit the model parameters using the training data, with a radial basis function as kernel\n",
    "- make predictions for the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Fitting model and comparing to sklearn KernelRidge\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from numpy.linalg import inv\n",
    "\n",
    "\n",
    "\n",
    "#Example values for the regularization and RBF variance hyper-parameters\n",
    "p_lambda = 0.1\n",
    "p_gamma = 0.01\n",
    "\n",
    "\n",
    "#RBF kernel for two matrices of points\n",
    "def gkernel(X1, X2, gamma):\n",
    "    pairwise_dists = np.square(cdist(X1,X2))\n",
    "    K = np.exp(-gamma* pairwise_dists)\n",
    "    return K\n",
    "\n",
    "\n",
    "# Build the kernel matrix for the training set\n",
    "K = \n",
    "print(K.shape)\n",
    "\n",
    "\n",
    "# Find the optimal alpha values with the closed form solution from 1.\n",
    "alpha = \n",
    "print(alpha.shape)\n",
    "\n",
    "# Find the kernel values for the test set\n",
    "K = \n",
    "print(K.shape)\n",
    "\n",
    "# Make predictions\n",
    "y_mine = \n",
    "print(y_mine.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Compare your results those of the in-built SVR function (in terms of the mean squared error) for the same values of regularization ($\\lambda$) and radial-basis function ($\\gamma$) hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Compute the mean squared error errors\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "print(\"Mean squared error MINE: %.2f\"% mean_squared_error(y_mine, y_test))\n",
    "print(y_mine)\n",
    "\n",
    "\n",
    "# built-in version with KernelRidge\n",
    "FILL IN HERE\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"% mean_squared_error(y_kr, y_test))\n",
    "print(y_kr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
