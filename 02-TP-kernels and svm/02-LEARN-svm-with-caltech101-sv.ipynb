{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Kernel methods and SVMs\n",
    "** APSTA - Ecole Centrale Nantes **\n",
    "\n",
    "** Ludivine Morvan, Diana Mateus **\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTICIPANTS: **(Fill in your names)**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage import io\n",
    "import random\n",
    "\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image classification on Caltech 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Download images from\n",
    "http://www.vision.caltech.edu/feifeili/Datasets.htm\n",
    "and run the code bellow to check the files and store the name of the classes in the list ```labelNamesAll```\n",
    "\n",
    "(Just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERIFY LOCATION AND STORE LABEL NAMES\n",
    "\n",
    "IMDIR = '101_ObjectCategories/'\n",
    "labelNamesAll = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(IMDIR):\n",
    "    labelNamesAll.append(dirnames)\n",
    "    #uncomment to check what is found in this folder\n",
    "    #for filename in filenames:\n",
    "        #f = os.path.join(root, filename)\n",
    "        #if f.endswith(('.png', '.jpg', '.jpeg','.JPG', '.tif', '.gif')):\n",
    "        #    print(f)\n",
    "\n",
    "labelNamesAll = labelNamesAll[0]\n",
    "\n",
    "#The list of all labels/directories is\n",
    "print(labelNamesAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Build a reduced dataset for accelerating process.** To do so: \n",
    "- Consider only up to $K$ randomly drawn categories (start with a binary case)\n",
    "- Read only up to $N$ images for each class\n",
    "- Resize the images to $(imWidth*imHeight)$\n",
    "\n",
    "The dataset should consist of a \n",
    "- Input matrix $\\mathbf{X}$ of size $(K\\cdot N)\\times (imWidth\\cdot imHeight)$ with one image in every row of the matrix. \n",
    "- Output vector $\\mathbf{y}$ of size $(K\\cdot N)\\times 1$ with the label index of each input point in $\\bf X$.\n",
    "- the reduced list of the label names of size $K$ to map between the indices and the names.\n",
    "\n",
    "**Note than different classes may have different number of images so that the actual number of $\\bf X$ and $\\bf y$ is less than $K*N$**\n",
    "\n",
    "(Run and understand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build DATASET from K categories and (up to) N images from category\n",
    "K = 2\n",
    "N = 40\n",
    "imWidth = 100\n",
    "imHeight = 100\n",
    "\n",
    "#selection of label indices\n",
    "X = np.zeros([K*N,imHeight*imWidth]) #data matrix, one image per row\n",
    "#Y = np.zeros([K*N,1]) #label indices\n",
    "Y = -np.ones([K*N,1]) #label indices\n",
    "labelNames = []\n",
    "\n",
    "#random.seed(a=42)\n",
    "\n",
    "\n",
    "globalCount = 0\n",
    "for i in range(K): \n",
    "    while True:\n",
    "        lab = random.randint(0,len(labelNamesAll)-1)\n",
    "        if lab not in labelNames:\n",
    "            break\n",
    "    #folders are named after the class label\n",
    "    filedir = os.path.join(IMDIR,labelNamesAll[lab])\n",
    "    print(filedir)\n",
    "\n",
    "    #save the name of the class\n",
    "    labelNames.append(labelNamesAll[lab])       \n",
    "\n",
    "    classCount = 0\n",
    "    for filename in os.listdir(filedir):\n",
    "        f = os.path.join(filedir, filename)\n",
    "        if f.endswith(('.jpg')) and (classCount < N):\n",
    "            image = skimage.io.imread(f, as_grey=True)\n",
    "            #image = skimage.io.imread(f, as_gray=True)\n",
    "            image = skimage.transform.resize(image, [imHeight,imWidth],mode='constant')#,anti_aliasing=True)\n",
    "            X[globalCount,:] = image.flatten()\n",
    "            Y[globalCount,:] = i\n",
    "            globalCount += 1\n",
    "            classCount += 1\n",
    "\n",
    "#Remove the unused entries of X and Y\n",
    "print(globalCount)\n",
    "X = X[:globalCount,:]\n",
    "Y = Y[:globalCount,:]\n",
    "\n",
    "#Check the stored classes\n",
    "print(labelNames)\n",
    "print(X.shape)\n",
    "print(Y.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Split the dataset into train (80% of samples) and test (20% samples). **\n",
    "(Fill in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.T)\n",
    "print(Y_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Create an SVC model, train it on the train set, and test it on the test set**. \n",
    "(Fill in and answer the questions)\n",
    "\n",
    "**Question** SVMs are intrinsically binary classifiers, can you train the SVC for K>2? How is that achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, train and test an svm model     \n",
    "\n",
    "\n",
    "print(\"True classes\",Y_test.T)\n",
    "print(\"Predictions\",Y_pred)\n",
    "errors = np.sum((Y_test.ravel()!=Y_pred))\n",
    "print('There were ', errors, 'errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Fill in the function bellow to computing different evaluation measures and give a performance report**\n",
    "Look at the formulas and definitions in https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n",
    "\n",
    "Start by computing the confusion matrix values TP, TN, FP, FN\n",
    "![image](confusion-matrix.png)\n",
    "\n",
    "**Question:** What are the different manners to compute and interpret the scores for multiple classes $K>2$ ? Implement them\n",
    "\n",
    "**Hint** Add a numerical zero eps to the denominators to prevent dividing by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute the errors between prediction and ground truth \n",
    "\n",
    "def compute_measures(Y_gt,Y_pred, positiveClass=1): #Y_gt = ground truth\n",
    "    measures = dict()\n",
    "    Y_len = len(Y_gt)\n",
    "    \n",
    "    eps = 1e-12\n",
    "    \n",
    "    # True positives TP\n",
    "    TP = \n",
    "        \n",
    "    # True negatives TN\n",
    "    TN = \n",
    "    \n",
    "    # False positives FP\n",
    "    FP = \n",
    "        \n",
    "    # False negatives FN\n",
    "    FN = \n",
    "\n",
    "    print('TP ', TP, 'TN ', TN, 'FP', FP, 'FN', FN, 'Total', TP+TN+FP+FN)\n",
    "    measures['TP'] = TP\n",
    "    measures['TN'] = TN\n",
    "    measures['FP'] = FP\n",
    "    measures['FN'] = FN\n",
    "    \n",
    "    \n",
    "    # Accuracy\n",
    "    measures['accuracy'] = \n",
    "    \n",
    "    # Precision\n",
    "    measures['precision'] = \n",
    "        \n",
    "    # Specificity\n",
    "    measures['specificity']=\n",
    "    \n",
    "    # Recall\n",
    "    measures['recall'] = \n",
    "    \n",
    "    # F-measure\n",
    "    measures['f1'] = \n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    measures['npv'] = \n",
    "    \n",
    "    # False Predictive Value\n",
    "    measures['fpr'] = \n",
    "    \n",
    "    print('Accuracy ', measures['accuracy'], '\\n',\n",
    "          'Precision', measures['precision'], '\\n',\n",
    "          'Recall', measures['recall'], '\\n',\n",
    "          'Specificity ', measures['specificity'], '\\n',\n",
    "          'F-measure', measures['f1'], '\\n',\n",
    "          'NPV', measures['npv'],'\\n',\n",
    "          'FPV', measures['fpr'],'\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    return measures\n",
    "\n",
    "def micro_average(measuresList):\n",
    "    microAverage = dict()\n",
    "    eps = 1e-12\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Accuracy\n",
    "    microAverage['accuracy'] = \n",
    "    \n",
    "    # Precision\n",
    "    microAverage['precision'] = \n",
    "        \n",
    "    # Specificity\n",
    "    microAverage['specificity'] = \n",
    "    \n",
    "    # Recall\n",
    "    microAverage['recall'] = \n",
    "    \n",
    "    # F-measure\n",
    "    microAverage['f1'] = \n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    microAverage['npv'] = \n",
    "    \n",
    "    # False Predictive Value\n",
    "    microAverage['fpr'] = \n",
    "    \n",
    "        \n",
    "    print('Accuracy ', microAverage['accuracy'], '\\n',\n",
    "          'Precision', microAverage['precision'], '\\n',\n",
    "          'Recall', microAverage['recall'], '\\n',\n",
    "          'Specificity ', microAverage['specificity'], '\\n',\n",
    "          'F-measure', microAverage['f1'], '\\n',\n",
    "          'NPV', microAverage['npv'],'\\n',\n",
    "          'FPV', microAverage['fpr'],'\\n')\n",
    "    \n",
    "    return microAverage\n",
    "\n",
    "def macro_average(measuresList):\n",
    "    macroAverage = dict()\n",
    "\n",
    "    # Accuracy\n",
    "    macroAverage['accuracy'] = \n",
    "    \n",
    "    # Precision\n",
    "    macroAverage['precision'] = \n",
    "        \n",
    "    # Specificity\n",
    "    macroAverage['specificity']= \n",
    "    \n",
    "    # Recall\n",
    "    macroAverage['recall'] = \n",
    "    \n",
    "    # F-measure\n",
    "    macroAverage['f1'] = \n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    macroAverage['npv'] = \n",
    "    \n",
    "    # False Predictive Value\n",
    "    macroAverage['fpr'] = \n",
    "    \n",
    "    print('Accuracy ', macroAverage['accuracy'], '\\n',\n",
    "          'Precision', macroAverage['precision'], '\\n',\n",
    "          'Recall', macroAverage['recall'], '\\n',\n",
    "          'Specificity ', macroAverage['specificity'], '\\n',\n",
    "          'F-measure', macroAverage['f1'], '\\n',\n",
    "          'NPV', macroAverage['npv'],'\\n',\n",
    "          'FPV', macroAverage['fpr'],'\\n')\n",
    "    \n",
    "    return macroAverage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** e)** Measure the performance of the SVC model for multiple classes $K>2$** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in a list of measure dictionaries taking as input a different positive class\n",
    "\n",
    "multiclass = []\n",
    "for k in range(K):\n",
    "    print('For class',labelNames[k])\n",
    "    multiclass.append(compute_measures(Y_test.ravel(),Y_pred, positiveClass=k))\n",
    "\n",
    "print('Macro-average')\n",
    "macro_average(multiclass)\n",
    "    \n",
    "print('Micro-average')\n",
    "micro_average(multiclass)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report #confusion_matrix, accuracy_score, precision_score, recall_score, f1_micro, f1_macro\n",
    "print(classification_report(Y_test.ravel(), Y_pred, target_names=labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) Show the test images as well as the the predictions (Y_pred) vs the ground truth (Y_gt) labels for the best model**\n",
    "(Just run for each analysed model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show some results\n",
    "width=20\n",
    "height=15\n",
    "plt.rcParams['figure.figsize'] = [width, height]\n",
    "fig=plt.figure()\n",
    "imCounter = 1\n",
    "for i in range(len(Y_test)):\n",
    "    image=np.reshape(X_test[i,:], (imHeight,imWidth)) \n",
    "\n",
    "    plt.subplot(5,7,imCounter)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    gtLabel = labelNames[Y_test.ravel()[i].astype(int)]\n",
    "    predLabel = labelNames[Y_pred.ravel()[i].astype(int)]\n",
    "    plt.title('GT: {}. \\n Pred: {}'.format(gtLabel, predLabel))\n",
    "\n",
    "    imCounter += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** g) REPORT:**  Change the hyperparameters of your SVC trying to optimize the F1 measure for different cases. Describe in your report the different variants of the model tried. Present and discuss your findings for different hyperparameters, number of classes and numbers of images. THIS IS THE MOST IMPORTANT PART FOR THE EVALUATION. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
