{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03- Ensembles\n",
    "** APSTA - LEARN - Ecole Centrale Nantes **\n",
    "\n",
    "** Diana Mateus **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTICIPANTS: **(Fill in your names)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision stumps\n",
    "A decision stump is a machine learning model consisting of a one-level decision tree. That is, it is a decision tree with one internal node (the root) which is immediately connected to the terminal nodes (its leaves). A decision stump makes a prediction based on the value of just a single input feature. Sometimes they are also called 1-rules [Wikipedia]\n",
    "\n",
    "***a)*** Run the provided code to generate and plot a toy dataset consisting of 2D points and 4 classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Load and plot dataset, split in train and test sets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_classes = 4\n",
    "X, y = make_blobs(n_samples=300, centers=n_classes,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.get_cmap('rainbow', 4));\n",
    "plt.colorbar();\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**b)** Implement a function that generates a random linear axis-aligned ***stump*** according to the number of features of a given dataset. The function should return \n",
    "    - the index of one randomly chosen features (dimension) \n",
    "    - as well as a randomly chosen threshold within the min and max values of the chosen feature.\n",
    "``` python\n",
    "def stump(Xtrain):\n",
    "```\n",
    "**c)**  Make a function **split** that \n",
    "    - receives as input the parameters generated by the stump function above. \n",
    "    - Partitions the **training** dataset in two subsets.\n",
    "The output should be two arrays, each containing the _indices_ of the points belonging to one or the other subset. To verify the implementation, run the split function several times, and display the resulting subsets as 2D pointsets with different colors.\n",
    "``` python\n",
    "def split(Xtrain, feature, threshold):\n",
    "```\n",
    "**d)** Implement a function ***leaf*** that computes and returns the class distribution (normalized histogram #points vs classes) \n",
    "    - of the original training set (before the split), \n",
    "    - for each of the 2 subsets resulting from after the split has been applied.\n",
    "\n",
    "``` python\n",
    "def split_distributions(ytrain,ind_left, ind_right):\n",
    "```\n",
    "**e)** Using the output of **split_distributions** Create a function ***information_gain*** that receives the full training dataset (Xtrain and ytrain) and the indices of two subsets resulting from the current split. Use the **entropy** to compute the information gain.\n",
    "``` python\n",
    "def information_gain(ytrain,ind_left, ind_right):\n",
    "```\n",
    "```Hints: ``` \n",
    "- a detailed example is given here: http://www.bogotobogo.com/python/scikit-learn/scikt_machine_learning_Decision_Tree_Learning_Informatioin_Gain_IG_Impurity_Entropy_Gini_Classification_Error.php\n",
    "- when computing the Entropy, ignore the classes with zero probabilities, and carry on summation using the same equation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**f)** Create a function ***train*** that receives as parameters the full training dataset (Xtrain and ytrain) as well as the number of random stumps to try. The function will \n",
    "- generate the desired number of stumps, \n",
    "- split the dataset according to each stump, \n",
    "- evaluate the ***information gain*** for each split\n",
    "- choose and then return the parameters of the best stump.\n",
    "\n",
    "``` python\n",
    "def train_stumps(Xtrain, ytrain, trials):\n",
    "```\n",
    "\n",
    "**g)** train an ensemble of stumps, average the prediction results and then print the performance (accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint, uniform\n",
    "\n",
    "# b) Stumps\n",
    "\n",
    "def stump(xx):\n",
    "    f =  #randomly select a feature/dimension randint()\n",
    "    t =  #sample from an uniform() between the min and max values of the feature\n",
    "    return f,t \n",
    "\n",
    "# c) Split\n",
    "\n",
    "def split(xx, f, t):\n",
    "    ind_l, = #points going to the left child\n",
    "    ind_r, = #points going to the right child\n",
    "    return ind_l, ind_r #return indices\n",
    "\n",
    "\n",
    "# d) Leaf probabilities\n",
    "def class_distribution(yy, n_classes):\n",
    "\n",
    "    return proba, bin_edges\n",
    "    \n",
    "def split_distributions(yy, ind_l, ind_r, n_classes):\n",
    "\n",
    "    \n",
    "    \n",
    "    return proba, proba_l, proba_r, bin_edges\n",
    "\n",
    "\n",
    "# e) information gain \n",
    "def entropy(p):\n",
    "\n",
    "    \n",
    "    return ent\n",
    "\n",
    "def information_gain(yy, ind_l, ind_r, n_clasess):\n",
    "    \n",
    "    return info_gain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------Runing functions from a, e-------------\n",
    "\n",
    "\n",
    "feat, th = stump(Xtrain)\n",
    "print('Stump parameters', feat, th)\n",
    "ind_l,ind_r = split(Xtrain, feat, th)\n",
    "print('Sizes of (original set, left subset, right subset))', len(ytrain), ind_l.shape, ind_r.shape)\n",
    "\n",
    "#Compute the information gain based measures\n",
    "p, p_l, p_r, bins = split_distributions(ytrain, ind_l, ind_r, n_classes)\n",
    "print('Info Gain', information_gain(ytrain,ind_l, ind_r, n_classes))\n",
    "\n",
    "#-------Plotting functions-------------\n",
    "fig, ax = plt.subplots(1,3,figsize=(12, 4))\n",
    "\n",
    "#Plot the training points with their ground truth class\n",
    "for c in range(n_classes):\n",
    "    X0_c=Xtrain[np.where(ytrain==c), 0]\n",
    "    X1_c=Xtrain[np.where(ytrain==c), 1]\n",
    "    y_c=c*np.ones(np.shape(Xtrain[np.where(ytrain==c),0]))\n",
    "    ax[0].scatter(X0_c,X1_c, s=50,alpha=0.5,cmap=plt.cm.get_cmap('rainbow', 4),label=str(c))\n",
    "\n",
    "ax[0].axis('equal')\n",
    "\n",
    "#draw the separation of the points by drawing a circle around the original point sets\n",
    "ax[0].scatter(Xtrain[ind_l, 0], Xtrain[ind_l, 1], c='none', edgecolor='r')\n",
    "ax[0].scatter(Xtrain[ind_r, 0], Xtrain[ind_r, 1], c='none', edgecolor='b')\n",
    "\n",
    "#draw a threshold line\n",
    "if feat == 0:\n",
    "    ax[0].plot([th,th],[np.min(Xtrain[:,1]),np.max(Xtrain[:,1])])\n",
    "elif feat == 1:\n",
    "    ax[0].plot([np.min(Xtrain[:,0]),np.max(Xtrain[:,0])],[th,th])\n",
    "leg = ax[0].legend();\n",
    "\n",
    "# Show the histograms over the classes for each subset\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "ax[1].bar(center, p_l, align='center')\n",
    "ax[1].set_ylim((0, 1.0))\n",
    "ax[1].set_title('Left child')\n",
    "ax[2].bar(center, p_r, align='center')\n",
    "ax[2].set_ylim((0, 1.0))\n",
    "ax[2].set_title('Right child')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#f) train a stump\n",
    "def train_stump(Xtrain, ytrain, trials):\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "def test_stump(Xtest, f, t, p_l, p_r):\n",
    "    \n",
    "    \n",
    "#---------------Run the functions------------------\n",
    "\n",
    "\n",
    "#---------------Plot results ------------------\n",
    "\n",
    "#Plot the ground thruth classes vs predictions\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "colors = np.random.random((len(ytest), 3))\n",
    "cmap=plt.cm.get_cmap('rainbow', n_classes)\n",
    "\n",
    "#draw predictions    \n",
    "for c in range(n_classes):\n",
    "    ind = np.where(y_pred==c)\n",
    "    X0_c=Xtest[ind, 0]\n",
    "    X1_c=Xtest[ind, 1]\n",
    "    y_c=c*np.ones([len(ind),0])\n",
    "    color = cmap(c)\n",
    "    ax.scatter(X0_c,X1_c, c='w',s=50,edgecolors=color,label=str(c))\n",
    "    \n",
    "#draw ground truth\n",
    "ax.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest, s=50, edgecolors='none', cmap=plt.cm.get_cmap('rainbow', 4),alpha=0.3);\n",
    "\n",
    "#draw threshold line\n",
    "if feat == 0:\n",
    "    ax.plot([th,th],[np.min(Xtrain[:,1]),np.max(Xtrain[:,1])])\n",
    "elif feat == 1:\n",
    "    ax.plot([np.min(Xtrain[:,0]),np.max(Xtrain[:,0])],[th,th])\n",
    "\n",
    "ax.axis('equal')\n",
    "leg = ax.legend();\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g) train a series of stumps, average the results and then print the performance (accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are an example of an *ensemble learner* built on decision trees. Decision trees are very intuitive:  ask a sequence of binary questions (stumps) to hierarchicaly split data points in different groups. The binary splitting makes the training and testing extremely efficient. The difficulty lies in choosing the *right questions* to ask. Is common to use a randomized optimization algorithm to explore different features and decide which questions (or \"splits\") provide the most information. The result is a very fast **non-parametric** classification. \n",
    "\n",
    "Use the provided code based IPython's ``interact`` and some functions from **fig_code** folder to visualize a decision tree in action and answer to the questions\n",
    "\n",
    "**a) Visualizing a decision tree**. Increase the depth and observe:\n",
    "- How are decision trees linked to the stumps trained above?\n",
    "- In which regions do new splits appear?\n",
    "- What happens with the splits as depth increases?\n",
    "\n",
    "**b)** Split the dataset in half and fit a decision tree to each half and visualize.\n",
    "- Describe the differences in the classification obtained with the two trees\n",
    "- What does this tell you about the bias and variance of the classifier?\n",
    "- When is this behaviour problematic? (Explain)\n",
    "\n",
    "**c) Bootstraping ** sample 100 subsets from the original dataset and give each to a different tree to train. Average and show the results for the full test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. a) Visualizing a decision tree\n",
    "\n",
    "import fig_code\n",
    "\n",
    "from fig_code import visualize_tree, plot_tree_interactive\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "\n",
    "plot_tree_interactive(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.b)  Subset \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[:200], y[:200], boundaries=False)\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[-200:], y[-200:], boundaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.c)  Subset \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "visualize_tree(clf, X, y, boundaries=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest for Classifying Digits (or Caltech 101)\n",
    "\n",
    "We will apply a Random Forest classifier to predict the classes of the **hand-written digits** dataset.\n",
    "\n",
    "**a)** Run the code to load and visualize the dataset\n",
    "\n",
    "**b)** Run the code to perform classification with a single decision tree\n",
    "\n",
    "**c)** Repeat the classification of the dataset\n",
    "- using a ``sklearn.ensemble.RandomForestClassifier``.  \n",
    "- How does the ``max_depth``, ``max_features``, and ``n_estimators`` affect the results? \n",
    "( Check and print the accuracy and F1 scores for different values of these parameters) \n",
    "\n",
    "**d)** Do a gridsearch with crossvalidation varying the three parameters above. \n",
    "- What are the best parameters to use for F1 score?\n",
    "- What's the best F1 score you can reach in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.a) Load and visualize dataset\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "print('Size of Data matrix', X.shape)\n",
    "print('Size of target vector', y.shape)\n",
    "\n",
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.b) Multiclass classification with a decision tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
    "clf = DecisionTreeClassifier(max_depth=11)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)\n",
    "\n",
    "#performance measure\n",
    "print(metrics.accuracy_score(ypred, ytest))\n",
    "\n",
    "#confusion matrix\n",
    "plt.imshow(metrics.confusion_matrix(ypred, ytest),\n",
    "           interpolation='nearest', cmap=plt.cm.binary)\n",
    "plt.grid(False)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.c) Classification with a Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. d) Grid search cross-validation\n",
    "#uncomment according to version\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "target_names = ['class 0', 'class 1', 'class 2', \n",
    "                'class 3', 'class 4', 'class 5', \n",
    "                'class 6', 'class 7', 'class 8', 'class 9']\n",
    "\n",
    "tuned_parameters = [{'max_depth': [1, 5, 10], \n",
    "                     'max_features': [1, 20, 50, 64],\n",
    "                     'n_estimators': [1, 10, 50, 100]}]\n",
    "\n",
    "gscv_model = GridSearchCV(RandomForestClassifier(max_depth=5, max_features = 1, n_estimators = 10),\n",
    "                          cv=5, param_grid = tuned_parameters, scoring='accuracy')\n",
    "gscv_model.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print grid search and cross validation results of each parameter set\n",
    "\n",
    "\n",
    "# Print best parameters and score\n",
    "print('Best params:', gscv_model.best_params_)\n",
    "print('Best score:', gscv_model.best_score_)\n",
    "\n",
    "# Predict on test set \n",
    "ypred = gscv_model.predict(Xtest)\n",
    "\n",
    "# Print compact report on test set\n",
    "print(metrics.classification_report(ytest, ypred, target_names = target_names))\n",
    "print(ytest)\n",
    "print(ypred)\n",
    "\n",
    "#performance measure\n",
    "print('Accuracy', metrics.accuracy_score(ypred, ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "33bef8a3ef034900959c512a6b116339": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
